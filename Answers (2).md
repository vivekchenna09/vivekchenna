1) (e) Comment on what you observe - how does the number of tokens change as you add more processing steps.

Answer : 
-> Token Count Decreases: As you perform various text processing steps, such as making words simpler, removing common words, and eliminating punctuation, the total number of words in your text usually decreases.
-> Cleaning and Simplifying: These steps are like cleaning and simplifying your text. You're making it neater and easier to understand.
-> Lemmatization: This process turns words like "running" into "run," which is shorter and simpler. It reduces variations of words to their basic form.
-> Removing Stop Words: Common words like "the," "and," and "in" are often removed. They don't carry much meaning, so they can be taken out.
-> Filtering Out Punctuation: Commas, dots, and other punctuation marks are often removed to make the text more focused and direct.
-> Reduced Vocabulary: All these steps combined lead to a smaller vocabulary or fewer unique words in your text.

2) (c) Comment on what you observe - how does the number of tokens change as you add more processing steps.

Answer : 

-> As you add more processing steps, such as converting text to lowercase, removing stop words, and eliminating punctuation marks, the number of tokens (unique terms) typically decreases. This is because these processing steps filter out non-essential words and reduce the dimensionality of the text data

3) Compare the tokens in the two books (number of tokens before/after processing, the most frequent and least frequent tokens) and describe your observations.

book1: Most repeated token: "said" (repeated 462 times)
book 2: Most repeated token: An empty token (represented as "[Blank]") repeated 16,036 times
book 1: Least repeated token: Various tokens, each appearing only once (e.g., "shepherd," "noises," "clamour," etc.)
book 2 : Least repeated token: Various tokens, each appearing only once (e.g., "director," "gbnewby@pglaf.org," "widest," etc.)
BOOK 1 : no of tokens before pre-processing 152174
BOOK 1 : no of tokens after pre-processing 13231
BOOK 1 : there is a loss of 138943 tokens 
BOOK 2 : no of tokens before pre-processing 1210513
BOOK 2 : no of tokens after pre-processing 115065
BOOK 2 : there is a loss of 1095448 tokens 
